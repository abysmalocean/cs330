# Lecture 1 Introduction and Overview

<p align = "center">   <img width = "500" src = "images/lecture1_2020-05-07-12-25-38.png "> </p>

## Homework 1

1. multi-task data processing, black-box meta-learning
2. Gradient-based meta-learning & metric learning
3. multi-task RL, goal relabeling

## why do we care about the deep multi-task & meta-learning learning? 

<p align = "center">   <img width = "500" src = "images/lecture1_2020-05-07-12-41-17.png "> </p>

Deep learning allows us to handle unstructured inputs without hand-engineering features, with less domain knowledge. 

why Multi task and meta-learning? 

what if you do not have a large dataset? 

<p align = "center">   <img width = "500" src = "images/lecture1_2020-05-07-12-45-03.png "> </p>

what if you need to quickly learn something new? 

what is a task
for now

1. dataset D
2. loss function L

and the model $f_\theta$

<p align = "center">   <img width = "500" src = "images/lecture1_2020-05-07-13-13-19.png "> </p>

**The multi-task learning problem**: learn all the tasks more quickly or more proficiently than learning them independent

**The meta-learning problem**: given data/experience on previous tasks, learn a new task more quickly and/or more proficlently. 

<p align = "center">   <img width = "500" src = "images/lecture1_2020-05-07-13-20-02.png "> </p>

<p align = "center">   <img width = "500" src = "images/lecture1_2020-05-07-13-20-11.png "> </p>

